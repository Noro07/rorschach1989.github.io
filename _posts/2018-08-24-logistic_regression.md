---
layout:     post
title:      A note on the two sides of logistic regression Statistical vs Learning theoretical 
mathjax:    true
---

# A minimal set up

Let's avoid making the formulation too restrictive: we observe $$ n $$ i.i.d. data points, each contains a binary label $$ Y \in \mathcal{Y} = \{0,1\} $$ and a $$ p- $$ dimensional covariate $$ X \in \mathcal{X} \subset \mathbb{R}^p $$. We want to use some model to characterize the relation between $$ Y $$ and $$ X $$, with the help of the sample $$ (Y_i, X_i), i = 1,\ldots,n $$  
The starting point of statisticians are simple, sleek and utterly convincing: \emph{Binary random variables are generated, were they are random, with some type of Bernoulli mechanism}. A Bernoulli variable $$ \xi \in \{0,1\} $$, using the mean parameter $$ \theta := \mathbb{P}(\xi = 1) \in (0,1) $$, is distributed with mass function  
$$
\begin{align}
    \mathbb{P}(\xi) = \theta^\xi (1-\theta)^{(1-\xi)} = \exp\left\lbrace  \xi \log \left( \frac{\theta}{1 - \theta}\right) - \log\left( \frac{1}{1 - \theta}  \right)\right\rbrace
\end{align}
$$  
the canonical parameter $$ \eta = \log (\frac{\theta}{1 - \theta}) $$ in the exponential family representation induced the \emph{logic link} in terms of GLM conventions. Note currently we haven't thought too much about the independent variables $$ X $$, but followed our instincts that $$ Y_i $$s are Bernoulli.\par
The first step toward incorporating $$ X $$ into the model may be the concern that $$ Y_i $$s are not \emph{homogeneous}, or there may be some underlying relationship between $$ Y_i $$s and their corresponding $$ X_i $$s, great, now we have 2 choices:

## Choice 1

Model $$ (X,Y) $$ jointly, like specify a model to describe $$ \mathbb{P}_{Y,X} (y,x) $$, this is indeed possible (like some Bayes approaches) but "heavy" since we are modeling a $$ p- $$ dimensional distribution, statisticians, especially frequentists don't like that, I suppose.

## Choice 2

Using conditional modeling, namely we decompose the randomness of the sample $$ (X,Y) $$ into 2 parts, those generated by $$ X $$ and by $$ Y | X $$.
For practical situations modeling the distribution of $$ X $$ appears way too ambitious, so we simplify the story by looking at the conditional relationship $$ Y | X $$, we made another assumption here, it turns out this assumption is kinda restrictive:
> Given $$ X_i $$s, $$ Y_i $$s are Bernoulli  

then we use the logit link to make a logistic regression model, here we assume the underlying relationship to be linear, that is:  
$$
\begin{align}
    \mathbb{P}(Y = 1 | X) = \dfrac{\exp(\langle \beta_0, X\rangle)}{1 + \exp(\langle \beta_0, X\rangle)}
\end{align}
$$  
After this formulation statisticians are very happy, since via estimation via maximum likelihood, we obtain a $$ \sqrt{n}- $$consistent, and asymptotically efficient estimator $$ \hat{\beta}_n $$. We could make hypothesis testings, construct confidence intervals, we have tons of beautiful stories to say about $$ Y $$ given $$ X $$
Among these stories maybe the one with much attention in practical is *prediction*, under which situation we shall evaluate our model via some metric, say \emph{classification error} if we use logistic regression to do classification. Here we shall state it clear about what we mean to "evaluate model with classification error": for any classification rule $$ f:  \mathcal{X} \mapsto \mathcal{Y} $$, the classification risk for rule $$ f $$ is defined as the expected loss with loss function chosen as the $$ 0-1 $$ loss, or  
$$
\begin{align}
    \mathcal{R}(f) = \mathbb{P}_{Y,X}\left( 1_{(Y \ne f(X))}\right) 
\end{align}
$$  
We may recall in the study of linear regression, where $ \mathcal{Y} = \mathbb{R} $ and we pick the $ \ell_2 $ loss, we get an optimal predictor over all square integrable functions of $ X $ to be $ \mathbb{E}(Y|X) $, here we could also derive the optimal over all binary valued functions, the optimal predictor is [5, Theorem 2.1]:  
$$
\begin{align}
    f^*(x) = 1_{\left( \mathbb{P}(Y = 1 | X = x) > 1/2\right) }
\end{align}
$$  
$$ f^* $$ is called *Bayes classifier* and $$ \mathcal{R}(f^*) $$ is called *Bayes risk*. Now we return to our logistic setup, suppose we specified the model correctly, and with a sufficiently large $$ n $$, we will have a $$ \hat{\beta}_n $$ very close to $$ \beta_0 $$ and a classifier almost identical to the Bayes classifier. in a asymptotic sense we may easily convince ourselves that the logistic classifier is asymptotically optimal.  
Then...it seems the story is over, we've made a persuasive story that logistic regression rocks (and indeed it is), so what's the point of machine learning people always saying "training with a cross entropy loss with softmax blablaba..."  

## Breaking the conditional Bernoulli assumption

Well, here's a very puzzling example, suppose $ Y $ is generated as 