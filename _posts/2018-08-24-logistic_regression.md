---
layout:     post
title:      A note on the two sides of logistic regression Statistical vs Learning theoretical 
mathjax:    true
author:     Ruofan Wu
---

# A minimal set up

Let's avoid making the formulation too restrictive: we observe $$ n $$ i.i.d. data points, each contains a binary label $$ Y \in \mathcal{Y} = \{0,1\} $$ and a $$ p- $$ dimensional covariate $$ X \in \mathcal{X} \subset \mathbb{R}^p $$. We want to use some model to characterize the relation between $$ Y $$ and $$ X $$, with the help of the sample $$ (Y_i, X_i), i = 1,\ldots,n $$  
The starting point of statisticians are simple, sleek and utterly convincing: \emph{Binary random variables are generated, were they are random, with some type of Bernoulli mechanism}. A Bernoulli variable $$ \xi \in \{0,1\} $$, using the mean parameter $$ \theta := \mathbb{P}(\xi = 1) \in (0,1) $$, is distributed with mass function  
$$
\begin{align}
    \mathbb{P}(\xi) = \theta^\xi (1-\theta)^{(1-\xi)} = \exp\left\lbrace  \xi \log \left( \frac{\theta}{1 - \theta}\right) - \log\left( \frac{1}{1 - \theta}  \right)\right\rbrace
\end{align}
$$  
the canonical parameter $$ \eta = \log (\frac{\theta}{1 - \theta}) $$ in the exponential family representation induced the \emph{logic link} in terms of GLM conventions. Note currently we haven't thought too much about the independent variables $$ X $$, but followed our instincts that $$ Y_i $$s are Bernoulli.\par
The first step toward incorporating $$ X $$ into the model may be the concern that $$ Y_i $$s are not \emph{homogeneous}, or there may be some underlying relationship between $$ Y_i $$s and their corresponding $$ X_i $$s, great, now we have 2 choices:

## Choice 1

Model $$ (X,Y) $$ jointly, like specify a model to describe $$ \mathbb{P}_{Y,X} (y,x) $$, this is indeed possible (like some Bayes approaches) but "heavy" since we are modeling a $$ p- $$ dimensional distribution, statisticians, especially frequentists don't like that, I suppose.

## Choice 2

Using conditional modeling, namely we decompose the randomness of the sample $$ (X,Y) $$ into 2 parts, those generated by $$ X $$ and by $$ Y | X $$.
For practical situations modeling the distribution of $$ X $$ appears way too ambitious, so we simplify the story by looking at the conditional relationship $$ Y | X $$, we made another assumption here, it turns out this assumption is kinda restrictive:
> Given $$ X_i $$s, $$ Y_i $$s are Bernoulli  

then we use the logit link to make a logistic regression model, here we assume the underlying relationship to be linear, that is:  
$$
\begin{align}
    \mathbb{P}(Y = 1 | X) = \dfrac{\exp(\langle \beta_0, X\rangle)}{1 + \exp(\langle \beta_0, X\rangle)}
\end{align}
$$  
After this formulation statisticians are very happy, since via estimation via maximum likelihood, we obtain a $$ \sqrt{n}- $$consistent, and asymptotically efficient estimator $$ \hat{\beta}_n $$. We could make hypothesis testings, construct confidence intervals, we have tons of beautiful stories to say about $$ Y $$ given $$ X $$
Among these stories maybe the one with much attention in practical is *prediction*, under which situation we shall evaluate our model via some metric, say \emph{classification error} if we use logistic regression to do classification. Here we shall state it clear about what we mean to "evaluate model with classification error": for any classification rule $$ f:  \mathcal{X} \mapsto \mathcal{Y} $$, the classification risk for rule $$ f $$ is defined as the expected loss with loss function chosen as the $$ 0-1 $$ loss, or  
$$
\begin{align}
    \mathcal{R}(f) = \mathbb{P}_{Y,X}\left( 1_{(Y \ne f(X))}\right) 
\end{align}
$$  
We may recall in the study of linear regression, where $$ \mathcal{Y} = \mathbb{R} $$ and we pick the $$ \ell_2 $$ loss, we get an optimal predictor over all square integrable functions of $$ X $$ to be $$ \mathbb{E}(Y|X) $$, here we could also derive the optimal over all binary valued functions, the optimal predictor is [5, Theorem 2.1]:  
$$
\begin{align}
    f^*(x) = 1_{\left( \mathbb{P}(Y = 1 | X = x) > 1/2\right) }
\end{align}
$$  
$$ f^* $$ is called *Bayes classifier* and $$ \mathcal{R}(f^*) $$ is called *Bayes risk*. Now we return to our logistic setup, suppose we specified the model correctly, and with a sufficiently large $$ n $$, we will have a $$ \hat{\beta}_n $$ very close to $$ \beta_0 $$ and a classifier almost identical to the Bayes classifier. in a asymptotic sense we may easily convince ourselves that the logistic classifier is asymptotically optimal.  
Then...it seems the story is over, we've made a persuasive story that logistic regression rocks (and indeed it is), so what's the point of machine learning people always saying "training with a cross entropy loss with softmax blablaba..."  

## Breaking the conditional Bernoulli assumption

Well, here's a very puzzling example, suppose $$ Y $$ is generated as 
$$
\begin{align}\label{weird}
    Y = 1_{\left( \langle \beta_0, X\rangle > 0 \right) }
\end{align}
$$  
This model is indeed possible in practical, like the label is generated via a stable *rule*, and given $ X_i $s, we don't have additional randomness here.  
This is a model that is extremely bad for statisticians, since $ \beta_0 $ is not identifiable. Moreover, even we want to use some joint modeling technique we would find that the corresponding likelihood equations are very uncomfortable to construct.  
Okay, one may say that no one knows *a priori* what the model class would be exactly like, this reminds one of the notion of some kind of *robustness*, however for an unidentifiable model, it would be complicated to establish results about using ordinary techniques to tackle them.  
Now let's stop worrying about the pessimistic side of the model, but some optimistic aspects  
> If we find a $$ \hat{\beta}_n $$ that is (not necessary arbitrarily close to as $$ N $$ grows) close to $$ \beta_0 $$, chances are we are making a classifier with almost $$ 0 $$ classification error  

If the above statement is achievable via some methods, we would be very happy as a practitioner, as long as we're not very much concerned about identifying the value of $$ \beta_0 $$, and this is why we need a *learning-theoretic* explanation of using logistic regression. And the answer to the previous case is pleasing: *we could simply use logistic regression to achieve this almost $ 0 $ classification error, with strong theoretical guarantees!*

## A learning model  

Here I adopt the approach from [7] and [2], see also a much richer review [3. The key fact here is that *analyzing logistic regression is not a trivial task in learning theory, at least in comparison to analyzing logistic regression in GLMs in statistics*. The analysis is a 2-stage procedure:  

### Step 1

We assume that we're able to solve a NP-Hard problem called empirical risk minimization w.r.t. $$ 0-1 $$ loss(Abbreviated ERM hereafter), and establish results about the minimizer  

### Step 2

We *relax* the ERM into a convex optimization problem (minimization with respect to logistic loss), and establish a property called *Fisher consistency* that guarantees using logistic regression is theoretically sound. We shall be very cautious here that the underlying model assumption could be completely wrong, so **the use of likelihood should be avoided**, although they're numerically equivalent, this methodological pitfall should be payed great attention by statisticians

## ERM, PAC, and uniform law of large numbers

First we list some basic ingredients, some of them are really an alias for statistical notions:  
**ERM** is like M-estimator in asymptotic statistics, using the previous notation, to minimize a population level risk  
$$
\begin{align*}
    \mathcal{R}(f) = \mathbb{P}_{Y,X}\left( l(Y, f(X))\right)
\end{align*}
$$  
one may resort to the minimization of the empirical risk
$$
\begin{align*}
    \mathcal{R}_n(f) = \mathbb{P}_n\left( l(Y, f(X))\right)
\end{align*}
$$  

Where $$ \mathbb{P}_n $$ is the expectation w.r.t. empirical distribution on $$ (Y_i, X_i), i=1,\ldots,n $$. And the domain of variable $$ f $$ is usually called \emph{Hypothesis space}, we denote it by $$ \mathcal{F} $$, and the ERM problem is defined as:  
$$
\begin{align}
    \hat{f}_n \in \arg\min_{f\in \mathcal{F}} \mathcal{R}_n(f)
\end{align}
$$  
For the binary classification using linear classifier task, $$ \mathcal{F}_{\text{lin}} = \left\lbrace f | f = 1_{(\langle \beta, X\rangle > 0 )}, \beta \in \mathbb{R}^p \right\rbrace  $$.  
**PAC** or **P**robably **A**pproximately **C**orrect, is an alternative characterization of *uniform Glivenko Cantelli* class [6, Chapter 10] with some involvement of ERM.. Originally proposed by Leslie Valiant in 1984 [9], this is a Turing award winning notion, and if we retrospect to 1980s, where the theory of empirical processes is just black magic known to a few of the probability masters like Rick Dudley. For characterizing the problem, I will not follow the 1984 paper but use the formulation in [7]: we say a hypothesis class $$ \mathcal{F} $$ is \emph{PAC-learnable} if for any *error* $$ \epsilon \in (0,1) $$ and any $$ \delta \in (0,1) $$, there exists a constant integer $$ m $$, only depends $$ \mathcal{F}, \epsilon, \delta $$ and denote it $$ m_{\mathcal{F}}(\epsilon, \delta) $$, such that there \emph{exists} an (learning) algorithm that, upon receiving $$ n > m_{\mathcal{F}}(\epsilon, \delta) $$ samples, the learning algorithm produces a result $$ \hat{f}_n $$ satisfying the event:
$$
\begin{align}\label{pac}
    \mathcal{R}(\hat{f}_n) \le \inf_{f\in \mathcal{F}}\mathcal{R}(f) + \epsilon
\end{align}
$$  
happens with probability greater than $$ 1-\delta $$, this $$ m_{\mathcal{F}}(\epsilon, \delta) $$ is referred to as **sample complexity**