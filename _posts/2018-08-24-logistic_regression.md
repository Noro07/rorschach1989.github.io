---
layout:     post
title:      A note on the two sides of logistic regression Statistical vs Learning theoretical 
mathjax:    true
---

# A minimal set up

Let's avoid making the formulation too restrictive: we observe $$ n $$ i.i.d. data points, each contains a binary label $$ Y \in \mathcal{Y} = \{0,1\} $$ and a $$ p- $$ dimensional covariate $$ X \in \mathcal{X} \subset \mathbb{R}^p $$. We want to use some model to characterize the relation between $$ Y $$ and $$ X $$, with the help of the sample $$ (Y_i, X_i), i = 1,\ldots,n $$  
The starting point of statisticians are simple, sleek and utterly convincing: \emph{Binary random variables are generated, were they are random, with some type of Bernoulli mechanism}. A Bernoulli variable $$ \xi \in \{0,1\} $$, using the mean parameter $$ \theta := \mathbb{P}(\xi = 1) \in (0,1) $$, is distributed with mass function  
$$
\begin{align}
    \mathbb{P}(\xi) = \theta^\xi (1-\theta)^{(1-\xi)} = \exp\left\lbrace  \xi \log \left( \frac{\theta}{1 - \theta}\right) - \log\left( \frac{1}{1 - \theta}  \right)\right\rbrace
\end{align}
$$  
the canonical parameter $$ \eta = \log (\frac{\theta}{1 - \theta}) $$ in the exponential family representation induced the \emph{logic link} in terms of GLM conventions. Note currently we haven't thought too much about the independent variables $$ X $$, but followed our instincts that $$ Y_i $$s are Bernoulli.\par
The first step toward incorporating $$ X $$ into the model may be the concern that $$ Y_i $$s are not \emph{homogeneous}, or there may be some underlying relationship between $$ Y_i $$s and their corresponding $$ X_i $$s, great, now we have 2 choices:

## Choice 1

Model $$ (X,Y) $$ jointly, like specify a model to describe $$ \mathbb{P}_{Y,X} (y,x) $$, this is indeed possible (like some Bayes approaches) but "heavy" since we are modeling a $$ p- $$ dimensional distribution, statisticians, especially frequentists don't like that, I suppose.

## Choice 2

Using conditional modeling, namely we decompose the randomness of the sample $$ (X,Y) $$ into 2 parts, those generated by $$ X $$ and by $$ Y | X $$.
For practical situations modeling the distribution of $$ X $$ appears way too ambitious, so we simplify the story by looking at the conditional relationship $$ Y | X $$, we made another assumption here, it turns out this assumption is kinda restrictive:
> Given $$ X_i $$s, $$ Y_i $$s are Bernoulli  
then we use the logit link to make a logistic regression model, here we assume the underlying relationship to be linear, that is:  
$$
\begin{align}
    \mathbb{P}(Y = 1 | X) = \dfrac{\exp(\langle \beta_0, X\rangle)}{1 + \exp(\langle \beta_0, X\rangle)}
\end{align}
$$  
After this formulation statisticians are very happy, since via estimation via maximum likelihood, we obtain a $$ \sqrt{n}- $$consistent, and asymptotically efficient estimator $$ \hat{\beta}_n $$. We could make hypothesis testings, construct confidence intervals, we have tons of beautiful stories to say about $$ Y $$ given $$ X $$
