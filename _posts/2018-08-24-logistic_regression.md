---
layout:     post
title:      A note on the two sides of logistic regression Statistical vs Learning theoretical 
mathjax:    true
author:     Ruofan Wu
---

# A minimal set up

Let's avoid making the formulation too restrictive: we observe $$ n $$ i.i.d. data points, each contains a binary label $$ Y \in \mathcal{Y} = \{0,1\} $$ and a $$ p- $$ dimensional covariate $$ X \in \mathcal{X} \subset \mathbb{R}^p $$. We want to use some model to characterize the relation between $$ Y $$ and $$ X $$, with the help of the sample $$ (Y_i, X_i), i = 1,\ldots,n $$  
The starting point of statisticians are simple, sleek and utterly convincing: \emph{Binary random variables are generated, were they are random, with some type of Bernoulli mechanism}. A Bernoulli variable $$ \xi \in \{0,1\} $$, using the mean parameter $$ \theta := \mathbb{P}(\xi = 1) \in (0,1) $$, is distributed with mass function  
$$
\begin{align}
    \mathbb{P}(\xi) = \theta^\xi (1-\theta)^{(1-\xi)} = \exp\left\lbrace  \xi \log \left( \frac{\theta}{1 - \theta}\right) - \log\left( \frac{1}{1 - \theta}  \right)\right\rbrace
\end{align}
$$  
the canonical parameter $$ \eta = \log (\frac{\theta}{1 - \theta}) $$ in the exponential family representation induced the \emph{logic link} in terms of GLM conventions. Note currently we haven't thought too much about the independent variables $$ X $$, but followed our instincts that $$ Y_i $$s are Bernoulli.\par
The first step toward incorporating $$ X $$ into the model may be the concern that $$ Y_i $$s are not \emph{homogeneous}, or there may be some underlying relationship between $$ Y_i $$s and their corresponding $$ X_i $$s, great, now we have 2 choices:

## Choice 1

Model $$ (X,Y) $$ jointly, like specify a model to describe $$ \mathbb{P}_{Y,X} (y,x) $$, this is indeed possible (like some Bayes approaches) but "heavy" since we are modeling a $$ p- $$ dimensional distribution, statisticians, especially frequentists don't like that, I suppose.

## Choice 2

Using conditional modeling, namely we decompose the randomness of the sample $$ (X,Y) $$ into 2 parts, those generated by $$ X $$ and by $$ Y | X $$.
For practical situations modeling the distribution of $$ X $$ appears way too ambitious, so we simplify the story by looking at the conditional relationship $$ Y | X $$, we made another assumption here, it turns out this assumption is kinda restrictive:
> Given $$ X_i $$s, $$ Y_i $$s are Bernoulli  

then we use the logit link to make a logistic regression model, here we assume the underlying relationship to be linear, that is:  
$$
\begin{align}
    \mathbb{P}(Y = 1 | X) = \dfrac{\exp(\langle \beta_0, X\rangle)}{1 + \exp(\langle \beta_0, X\rangle)}
\end{align}
$$  
After this formulation statisticians are very happy, since via estimation via maximum likelihood, we obtain a $$ \sqrt{n}- $$consistent, and asymptotically efficient estimator $$ \hat{\beta}_n $$. We could make hypothesis testings, construct confidence intervals, we have tons of beautiful stories to say about $$ Y $$ given $$ X $$
Among these stories maybe the one with much attention in practical is *prediction*, under which situation we shall evaluate our model via some metric, say \emph{classification error} if we use logistic regression to do classification. Here we shall state it clear about what we mean to "evaluate model with classification error": for any classification rule $$ f:  \mathcal{X} \mapsto \mathcal{Y} $$, the classification risk for rule $$ f $$ is defined as the expected loss with loss function chosen as the $$ 0-1 $$ loss, or  
$$
\begin{align}
    \mathcal{R}(f) = \mathbb{P}_{Y,X}\left( 1_{(Y \ne f(X))}\right) 
\end{align}
$$  
We may recall in the study of linear regression, where $$ \mathcal{Y} = \mathbb{R} $$ and we pick the $$ \ell_2 $$ loss, we get an optimal predictor over all square integrable functions of $$ X $$ to be $$ \mathbb{E}(Y|X) $$, here we could also derive the optimal over all binary valued functions, the optimal predictor is [5, Theorem 2.1]:  
$$
\begin{align}
    f^*(x) = 1_{\left( \mathbb{P}(Y = 1 | X = x) > 1/2\right) }
\end{align}
$$  
$$ f^* $$ is called *Bayes classifier* and $$ \mathcal{R}(f^*) $$ is called *Bayes risk*. Now we return to our logistic setup, suppose we specified the model correctly, and with a sufficiently large $$ n $$, we will have a $$ \hat{\beta}_n $$ very close to $$ \beta_0 $$ and a classifier almost identical to the Bayes classifier. in a asymptotic sense we may easily convince ourselves that the logistic classifier is asymptotically optimal.  
Then...it seems the story is over, we've made a persuasive story that logistic regression rocks (and indeed it is), so what's the point of machine learning people always saying "training with a cross entropy loss with softmax blablaba..."  

## Breaking the conditional Bernoulli assumption

Well, here's a very puzzling example, suppose $$ Y $$ is generated as 
$$
\begin{align}\label{weird}
    Y = 1_{\left( \langle \beta_0, X\rangle > 0 \right) }
\end{align}
$$  
This model is indeed possible in practical, like the label is generated via a stable *rule*, and given $ X_i $s, we don't have additional randomness here.  
This is a model that is extremely bad for statisticians, since $ \beta_0 $ is not identifiable. Moreover, even we want to use some joint modeling technique we would find that the corresponding likelihood equations are very uncomfortable to construct.  
Okay, one may say that no one knows *a priori* what the model class would be exactly like, this reminds one of the notion of some kind of *robustness*, however for an unidentifiable model, it would be complicated to establish results about using ordinary techniques to tackle them.  
Now let's stop worrying about the pessimistic side of the model, but some optimistic aspects  
> If we find a $$ \hat{\beta}_n $$ that is (not necessary arbitrarily close to as $$ N $$ grows) close to $$ \beta_0 $$, chances are we are making a classifier with almost $$ 0 $$ classification error  

If the above statement is achievable via some methods, we would be very happy as a practitioner, as long as we're not very much concerned about identifying the value of $$ \beta_0 $$, and this is why we need a *learning-theoretic* explanation of using logistic regression. And the answer to the previous case is pleasing: *we could simply use logistic regression to achieve this almost $ 0 $ classification error, with strong theoretical guarantees!*

## A learning model  

Here I adopt the approach from [7] and [2], see also a much richer review [3. The key fact here is that *analyzing logistic regression is not a trivial task in learning theory, at least in comparison to analyzing logistic regression in GLMs in statistics*. The analysis is a 2-stage procedure:  

### Step 1

We assume that we're able to solve a NP-Hard problem called empirical risk minimization w.r.t. $$ 0-1 $$ loss(Abbreviated ERM hereafter), and establish results about the minimizer  

### Step 2

We *relax* the ERM into a convex optimization problem (minimization with respect to logistic loss), and establish a property called *Fisher consistency* that guarantees using logistic regression is theoretically sound. We shall be very cautious here that the underlying model assumption could be completely wrong, so **the use of likelihood should be avoided**, although they're numerically equivalent, this methodological pitfall should be payed great attention by statisticians

## ERM, PAC, and uniform law of large numbers

First we list some basic ingredients, some of them are really an alias for statistical notions:  
**ERM** is like M-estimator in asymptotic statistics, using the previous notation, to minimize a population level risk  
$$
\begin{align*}
    \mathcal{R}(f) = \mathbb{P}_{Y,X}\left( l(Y, f(X))\right)
\end{align*}
$$  
one may resort to the minimization of the empirical risk
$$
\begin{align*}
    \mathcal{R}_n(f) = \mathbb{P}_n\left( l(Y, f(X))\right)
\end{align*}
$$  

Where $$ \mathbb{P}_n $$ is the expectation w.r.t. empirical distribution on $$ (Y_i, X_i), i=1,\ldots,n $$. And the domain of variable $$ f $$ is usually called \emph{Hypothesis space}, we denote it by $$ \mathcal{F} $$, and the ERM problem is defined as:  
$$
\begin{align}
    \hat{f}_n \in \arg\min_{f\in \mathcal{F}} \mathcal{R}_n(f)
\end{align}
$$  
For the binary classification using linear classifier task, $$ \mathcal{F}_{\text{lin}} = \left\lbrace f | f = 1_{(\langle \beta, X\rangle > 0 )}, \beta \in \mathbb{R}^p \right\rbrace  $$.  
**PAC** or **P**robably **A**pproximately **C**orrect, is an alternative characterization of *uniform Glivenko Cantelli* class [6, Chapter 10] with some involvement of ERM.. Originally proposed by Leslie Valiant in 1984 [9], this is a Turing award winning notion, and if we retrospect to 1980s, where the theory of empirical processes is just black magic known to a few of the probability masters like Rick Dudley. For characterizing the problem, I will not follow the 1984 paper but use the formulation in [7]: we say a hypothesis class $$ \mathcal{F} $$ is \emph{PAC-learnable} if for any *error* $$ \epsilon \in (0,1) $$ and any $$ \delta \in (0,1) $$, there exists a constant integer $$ m $$, only depends $$ \mathcal{F}, \epsilon, \delta $$ and denote it $$ m_{\mathcal{F}}(\epsilon, \delta) $$, such that there \emph{exists} an (learning) algorithm that, upon receiving $$ n > m_{\mathcal{F}}(\epsilon, \delta) $$ samples, the learning algorithm produces a result $$ \hat{f}_n $$ satisfying the event:
$$
\begin{align}\label{pac}
    \mathcal{R}(\hat{f}_n) \le \inf_{f\in \mathcal{F}}\mathcal{R}(f) + \epsilon
\end{align}
$$  
happens with probability greater than $$ 1-\delta $$, this $$ m_{\mathcal{F}}(\epsilon, \delta) $$ is referred to as **sample complexity**  

### Realizable case

now we quickly take a look at (\ref{weird}), here using the hypothesis class $$ \mathcal{F}_{\text{lin}} $$, we realize that $$ \inf_{f\in \mathcal{F}_{\text{lin}}}\mathcal{R}(f) = 0 $$, meaning that if $$ \mathcal{F}_{\text{lin}}$$ is PAC-learnable and we have the sample complexity $$ m_{\mathcal{F}_{\text{lin}}}(\epsilon, \delta) $$, than we can achieve arbitrary low error with high confidence with a sample size we're can calculate. This is a special case in learning theory called *realizable case* in [7]

## ULLN & VC classes

for PAC learnable classes, we throw directly the answer here that **ERM is a learning algorithm that satisfies the PAC condition (\ref{pac})**. If one's familiar with the consistency proof of MLE, especially the Wald-type argument like in [10], it's not hard to verify that it suffices to bound the uniform deviation  
$$
\begin{align}\label{ulln}
    \sup_{f \in \mathcal{F}}\left( \mathbb{P}_n - \mathbb{P}\right)f 
\end{align}
$$  
Actually it's easy to verify the inequality $$ \mathcal{R}(\hat{f}_n) -  \inf_{f\in \mathcal{F}}\mathcal{R}(f) \le 2\sup_{f \in \mathcal{F}}\left| \left( \mathbb{P}_n - \mathbb{P}\right)f \right|  $$  
Moreover, since the sample complexity function required by PAC is independent of the generating distribution $$ \mathbb{P} $$, the argument (\ref{ulln}) should somehow be uniform over all probability distributions $$ \mathbb{P} \in \mathcal{P}  $$, where $$ \mathcal{P} $$ denotes the collection of all legal probability distributions on the sample space. For this to hold, we require $ \mathcal{F} $ to be a VC class, see for example, [6, Theorem 6.27]. In the textbook [7] the authors summarize these facts into a theorem referred as **fundamental theorem of machine learning**. In short, this theorem says that:  
>A hypothesis class $$ \mathcal{F} $$ is PAC learnable $$ \Longleftrightarrow $$ $$ \mathcal{F} $$ is a VC class  

This is not a trivial result in empirical process theory, but note that decision functions for classification problems are uniformly bounded by $$ 1 $$, see a detailed discussion in Chapter 10 of [6], and ERM is guaranteed to produce the prediction rule  

### Limitedness of PAC

PAC is the most restrictive learning model in that the sample complexity is required to be independent of both the generating distribution $ \mathbb{P} $ and any specific points $$ f \in \mathcal{F} $$. For example most of the nonparametric regression schemes are not PAC since they adopt a class to approximate a very big function class (like $$ C_{[0,1]} $$) in a progressive way. There are methods for extending and relaxing the PAC assumption, while they are not of main concern here so I omit them}  

So up till now, suppose we're able to solve ERM, we're able to build a "good" algorithm that at least asymptotically will lead us to the best risk we could achieve (Bayes risk), and if we want to make it precise how much sample point is needed (sample complexity), we shall analyze the rate of convergence of the supreme of empirical process, this is of huge effort. The renowned Vapnik-\v{C}hervonenkis inequality [3, Theorem 3.4]  tells us that the sample complexity would be of the order $$ O\left(  \dfrac{V + \log(1/\delta)}{\epsilon^2}\right)  $$, where $$ V $$ is the VC-dimension of $$  \mathcal{F} $$. The VC-dimension is actually VC-index $$ -1 $$, a very slight definition difference from van der Vaart and Wellner[11]. However, **ERM is not solvable, at least in reasonable time complexity**[1]  

# Convex surrogates and Fisher consistency

We didn't touch logistic regression in the ERM setting, and now we claim that l**ogistic regression is asymptotically as good as ERM, but for finite sample we should pay some price for an "inexact" loss function**

## Convex surrogate losses

I think the best way to characterize this concept is to use the graph in [2]  
![The above graph shows several useful **convex surrogates** for the $ 0-1 $ loss in binary classification setting. ]({{ "/assets/surrogates.png" | absolute_url }})
> Here the label space is altered as $$ \mathcal{Y} = \{-1, 1\} $$, so the logistic loss defined as $$ l_{\text{logistic}}(y,f(x)) = \log\left( 1 + \exp(-yf(x))\right)  $$ is equivalent to Bernoulli likelihood. In this graph, the loss function is further reduced in terms of input variables are associated with the loss via the product $$ yf(x) $$, thus the plot use representations like $$ \phi_{\text{logistic}}(\alpha) = \log\left( 1 + \exp(-\alpha)\right)  $$  

Now with the change of loss function, we are facing with another risk minimization scheme called convex surrogate risk minimization, which again is an M-estimation: suppose we used a convex function $$ \phi: \mathbb{R} \mapsto [0, \infty) $$ to replace the $$ 0-1 $$ loss in a way that $$ l_{\phi}(y,f(x)) = \phi(yf(x)) $$, then it's trivial to define the corresponding surrogate risks:  
$$
\begin{align}
    \mathcal{R}_{\phi}(f) &= \mathbb{P}_{Y,X}\left( \phi(Yf(X))\right) \\
    \widehat{\mathcal{R}}_{\phi}(f) &= \mathbb{P}_n\left( \phi(Yf(X))\right)
\end{align}
$$  

**Fisher consistency of $$ \phi $$** characterizes the *correctness of using the convex surrogate $$ \phi $$* in that $$ \phi $$ is Fisher consistent for the $$ 0-1 $$ loss if for any sequence of measurable functions $$ f_n : \mathcal{X} \mapsto \mathbb{R} $$ and every distribution $$ \mathbb{P}_{X,Y} $$ defined on the sample space:  
> $$ \mathcal{R}_{\phi}(f_n) \rightarrow \inf_{f\in \mathcal{F}}\mathcal{R}_{\phi}(f)  $$ implies that $$ \mathcal{R}(f_n) \rightarrow \inf_{f\in \mathcal{F}}\mathcal{R}(f) $$  

Now the mission is clear, if we could somehow find a sufficient condition for $ \phi $ to be Fisher-consistent, than we need only verify it for logistic loss.  
> Here we suppress a step that the empirical surrogate risk minimizer is consistent, i.e.
>
$$
\begin{align}
    \mathcal{R}_{\phi}(\hat{f}^\phi_n) \overset{\mathbb{P}}{\rightarrow} \inf_{f\in \mathcal{F}}\mathcal{R}_{\phi}(f), \hat{f}^\phi_n = \arg\max_{f\in \mathcal{F}}\widehat{\mathcal{R}}_{\phi}(f)
\end{align}
$$  

while for statisticians I think this is just M-estimation if you don't care too much about finite sample precisions, a combined result is in [2]
In fact [2, Theorem 1] gave a \emph{necessary and sufficient condition} called \emph{classification-calibrated}, the definition is also kinda intuitive, but also sophisticated:

### Classification-calibrated functions

First we make a conditioning argument on $ X $, so that  
$$
\begin{align}
    &\mathbb{P}_{Y|X}\left( \phi(yf(X))| X = x\right) = \eta(x) \phi(f(x)) + (1-\eta(x))\phi(-f(x)), \notag \\
    &\eta(x) := \mathbb{P}_{Y,X}(Y=1| X=x)
\end{align}
$$  
Then we drop the restriction on $ x $ by introducing:  
$$
\begin{align}
    C_{\eta}(\alpha) = \eta \phi(\alpha) + (1 - \eta) \phi(-\alpha)
\end{align}
$$  
We use $$ C_{\eta} $$ to further define 2 terms that measures somehow the "discriminative power" of $$ \phi $$  
$$  
\begin{align}
    H(\eta) &= \inf_{\alpha \in \mathbb{R}} C_{\eta}(\alpha) \\
    H^-(\eta) &= \inf_{\alpha: \alpha(2\eta - 1) \le 0}C_{\eta}(\alpha)
\end{align}
$$  
The term $$ H(\eta) $$ is the "best risk" we could achieve, and $$ H^-(\eta) $$ is the "best risk" we could achieve *given our prediction is wrong*. One shall surely expect $$ H^-(\eta) $$ is always greater than $$ H(\eta) $$ except at $$ \eta = 1/2 $$ , and this is called \emph{classification-calibrated} property of $$ \phi $$  

## Fisher consistency for logistic loss

for investigating $$ \phi_{\text{logistic}}(\alpha) = \log\left( 1 + \exp(-\alpha)\right)  $$, easy calculus shows that $$ H(\eta) = \eta \log \dfrac{1}{\eta} + (1 - \eta) \log \dfrac{1}{1 - \eta} $$ and $$ H^-(\eta) \equiv \log2 $$, then Jensen's inequality implies $$ \phi_{\text{logistic}} $$ is classification calibrated. Cool, we're perfectly fine with logistic regression
**Finite sample characterization** we've showed that asymptotically logistic regression would give us the best predictor, while for a more refined finite sample analysis, it's also possible to give a PAC-like characterization of convex surrogate problems. [2] used a notion of $$ \psi- $$ transform for relating the two excess risks in a way like  
$$
\begin{align}
    \psi \left( \mathcal{R}(f_n) - \inf_{f\in \mathcal{F}}\mathcal{R}(f)\right) \le \mathcal{R}_{\phi}(f_n) - \inf_{f\in \mathcal{F}}\mathcal{R}_{\phi}(f)
\end{align}
$$  
While the precise definition of the $ \psi- $ transform involves elegant convex analysis, I recommend reading that paper for details  

# Wrap up

## So far, so good

Hopefully the above discussion has made myself clear about the legitimacy of using logistic regression, to briefly recap:

+ As long as there are presumed randomness in our data, using logistic regression is fine  
+ For statisticians, if the conditional Bernoulli assumption is indeed correct (I think this could be clarified in many cases), we're confident of applying this technique for all kinds of tasks, inferential or predictive  
+ For machine learning scientists, if we could forget about the precise significance of the model parameter and just make a black-box predictor, logistic regression is still theoretically sound

## Practical cases

when logistic regression (linear in input features) turns out to be a bad predictor, logistic assumption is indeed not to blame, as there's nothing more intuitive than it for binary data. Furthermore, checking the conditional Bernoulli assumption is difficult. This phenomenon was discussed by the ingenious Leo Breiman [4] on his highly influential, yet a little bit confrontational view on the statistician's way of modeling data. That paper is timelessly classic, a must read for every statistician, and see also the discussions by the world's greatest statisticians like D.R.Cox and Brad Efron on defense of elegant statistical modeling  

On the other hand, we're fully aware of what hypothesis class we are using ($$ \mathcal{F}_{\text{lin}}$$), so one cause should be our *approximation error* is high, i.e. the Bayes risk is large. Then efforts should be made toward data mining for a larger and more expressive feature set. Another possibility is to make feature transformations to increase the approximation power in view of some function spaces (like using polynomials), both approaches bear the risk of blow up the complexity (i.e. VC-dimension) of the hypothesis class, this is essentially the bias-variance trade off

## Learning theory and empirical processes

it is regrettable for a statistician to not take a look at learning theory if he/she's been struggling many years on empirical processes. The main difference with what we use in our proofs is that van der Vaart and Wellners' book uses empirical processes mainly for proving asymptotic results, while learning theory emphasizes on \emph{finite sample results}, which leans toward the use of *concentration inequalities*. If you read [11] carefully, there's plenty of concentration results (and they are indeed the most useful ones) stated in Chapter 2, but in a somehow unfriendly way.  
Recall that, [11] defined Orlitz norms before introducing Hoeffding's inequality... van der Vaart likes to stab right into the heart of the problem, while there still exist readers like me are not that smart...}
To this end, I found Roman Vershynin's new book [12] helpful and readable

## References

[1]. Arora, S., Babai, L., Stern, J., and Sweedyk, Z. The hardness of approximate optima in lattices, codes, and systems of linear equations. In Foundations of Computer Science, 1993. Proceedings., 34th Annual Symposium on (1993), IEEE, pp. 724–733.  
[2]. Bartlett, P. L., Jordan, M. I., and McAuliffe, J. D. Convexity, classification, and risk bounds. Journal of the American Statistical Association 101, 473 (2006), 138–156.  
[3]. Boucheron, S., Bousquet, O., and Lugosi, G. Theory of classification: A survey of some recent advances. ESAIM: probability and statistics 9 (2005), 323–375.  
[4]. Breiman, L., et al. Statistical modeling: The two cultures (with comments and a rejoinder by the author). Statistical science 16, 3 (2001), 199–231.  
[5]. Devroye, L., Györfi, L., and Lugosi, G. A probabilistic theory of pattern recognition, vol. 31. Springer Science & Business Media, 2013.  
[6]. Dudley, R. M. Uniform Central Limit Theorems, 2nd ed. Cambridge University Press, Cambridge, 2014.  
[7]. Shalev-Shwartz, S., and Ben-David, S. Understanding machine learning: From theory to algorithms. Cambridge university press, 2014.  
[8]. Tsiatis, A. Semiparametric theory and missing data. Springer Science & Business Media, 2007.  
[9]. Valiant, L. G. A theory of the learnable. Communications of the ACM 27, 11 (1984), 1134–1142.  
[10]. Van der Vaart, A. W. Asymptotic statistics, vol. 3. Cambridge university press, 2000.  
[11]. van der Vaart, A. W., and Wellner, J. A. Weak Convergence and Empirical Processes. Springer-Verlag, New York, 1996.  
[12]. Vershynin, R. High-Dimensional Probability.